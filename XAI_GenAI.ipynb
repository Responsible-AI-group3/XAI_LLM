{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible AI: XAI GenAI project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Background\n",
    "\n",
    "\n",
    "\n",
    "Based on the previous lessons on explainability, post-hoc methods are used to explain the model, such as saliency map, SmoothGrad, LRP, LIME, and SHAP. Take LRP (Layer Wise Relevance Propagation) as an example; it highlights the most relevant pixels to obtain a prediction of the class \"cat\" by backpropagating the relevance. (image source: [Montavon et. al (2016)](https://giorgiomorales.github.io/Layer-wise-Relevance-Propagation-in-Pytorch/))\n",
    "\n",
    "<!-- %%[markdown] -->\n",
    "![LRP example](images/catLRP.jpg)\n",
    "\n",
    "Another example is about text sentiment classification, here we show a case of visualizing the importance of words given the prediction of 'positive':\n",
    "\n",
    "![text example](images/textGradL2.png)\n",
    "\n",
    "where the words highlight with darker colours indicate to be more critical in predicting the sentence to be 'positive' in sentiment.\n",
    "More examples could be found [here](http://34.160.227.66/?models=sst2-tiny&dataset=sst_dev&hidden_modules=Explanations_Attention&layout=default).\n",
    "\n",
    "Both cases above require the class or the prediction of the model. But:\n",
    "\n",
    "***How do you explain a model that does not predict but generates?***\n",
    "\n",
    "In this project, we will work on explaining the generative model based on the dependency between words. We will first look at a simple example, and using Point-wise Mutual Information (PMI) to compute the saliency map of the sentence. After that we will contruct the expereiment step by step, followed by exercises and questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A simple example to start with\n",
    "Given a sample sentence: \n",
    "> *Tokyo is the capital city of Japan.* \n",
    "\n",
    "We are going to explain this sentence by finding the dependency using a saliency map between words.\n",
    "The dependency of two words in the sentence could be measured by [Point-wise mutual information (PMI)](https://en.wikipedia.org/wiki/Pointwise_mutual_information): \n",
    "\n",
    "\n",
    "Mask two words out, e.g. \n",
    "> \\[MASK-1\\] is the captial city of \\[MASK-2\\].\n",
    "\n",
    "\n",
    "Ask the generative model to fill in the sentence 10 times, and we have:\n",
    "\n",
    "| MASK-1      | MASK-2 |\n",
    "| ----------- | ----------- |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  paris  |     france    |\n",
    "|  beijing |  china |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  beijing |  china |\n",
    "\n",
    "PMI is calculated by: \n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "where $x$, $y$ represents the words that we masked out, $s$ represents the setence, and $s-\\{x,y\\}$ represents the sentences tokens after removing the words $x$ and $y$.\n",
    "\n",
    "In this example we have $PMI(Tokyo, capital) = log_2 \\frac{0.2}{0.2 * 0.2} = 2.32$\n",
    "\n",
    "Select an interesting word in the sentences; we can now compute the PMI between all other words and the chosen word using the generative model:\n",
    "(Here, we use a longer sentence and run 20 responses per word.)\n",
    "![](images/resPMI.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation\n",
    "### 2.1 Conda enviroment\n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate xai_llm\n",
    "```\n",
    "\n",
    "\n",
    "### 2.2 Download the offline LLM\n",
    "\n",
    "We use the offline LLM model from hugging face. It's approximately 5 GB.\n",
    "Download it using the comman below, and save it under `./models/`.\n",
    "```\n",
    "huggingface-cli download TheBloke/openchat-3.5-0106-GGUF openchat-3.5-0106.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "# credit to https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mask the sentence and get the responses from LLM\n",
    "### 3.1 Get the input sentence\n",
    "\n",
    "**Remember to change the anchor word index when changing the input sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# change to your working directory\n",
    "os.chdir(R'C:\\Users\\anned\\OneDrive - Danmarks Tekniske Universitet\\Uni\\Responible AI\\Project\\Project_3\\XAI_LLM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  The quick brown fox\n"
     ]
    }
   ],
   "source": [
    "def get_input():\n",
    "    # ideally this reads inputs from a file, now it just takes an input\n",
    "    return input(\"Enter a sentence: \")\n",
    "    \n",
    "anchor_word_idx = 3 # the index of the interested word\n",
    "prompts_per_word = 20 # number of generated responses  \n",
    "\n",
    "#sentence = get_input()\n",
    "# I chose to ignore the input function and just set the sentence manually because why wouldn't you?\n",
    "# Test with this shorter sentence to make code run faster. I later load in the full responses for the full sentence, but this takes a long time to run.\n",
    "sentence = \"The quick brown fox\"# jumps over the lazy dog\"\n",
    "print(\"Sentence: \", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: openchat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.name': 'openchat_openchat-3.5-0106', 'general.architecture': 'llama', 'llama.context_length': '8192', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\"}\n"
     ]
    }
   ],
   "source": [
    "from models.ChatModel import ChatModel\n",
    "model_name = \"openchat\"\n",
    "model = ChatModel(model_name)\n",
    "print(f\"Model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run the prompts and get all the responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] quick brown [MASK]:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] quick brown [MASK]:   5%|▌         | 1/20 [00:12<04:04, 12.87s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  10%|█         | 2/20 [00:19<02:42,  9.04s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  15%|█▌        | 3/20 [00:21<01:44,  6.15s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  20%|██        | 4/20 [00:28<01:38,  6.18s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  25%|██▌       | 5/20 [00:32<01:20,  5.37s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  30%|███       | 6/20 [00:35<01:05,  4.68s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  35%|███▌      | 7/20 [00:42<01:11,  5.52s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  40%|████      | 8/20 [00:51<01:19,  6.60s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  45%|████▌     | 9/20 [01:02<01:28,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'quick', 'brown', '[mask]'] ['the', 'fox', 'jumps', 'quickly', 'over', 'the', 'lazy', 'dog']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  50%|█████     | 10/20 [01:08<01:12,  7.28s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  55%|█████▌    | 11/20 [01:11<00:55,  6.16s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  60%|██████    | 12/20 [01:19<00:53,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'quick', 'brown', '[mask]'] ['the', 'fox', 'is', 'quite', 'fast']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  65%|██████▌   | 13/20 [01:33<01:01,  8.75s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  70%|███████   | 14/20 [01:38<00:46,  7.68s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  75%|███████▌  | 15/20 [01:48<00:41,  8.37s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  80%|████████  | 16/20 [01:57<00:33,  8.43s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  85%|████████▌ | 17/20 [02:02<00:22,  7.50s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  90%|█████████ | 18/20 [02:10<00:15,  7.79s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]:  95%|█████████▌| 19/20 [02:15<00:06,  6.71s/it]Llama.generate: prefix-match hit\n",
      "Input: [MASK] quick brown [MASK]: 100%|██████████| 20/20 [02:20<00:00,  7.03s/it]\n",
      "Input: The [MASK] brown [MASK]:   0%|          | 0/20 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:   5%|▌         | 1/20 [00:07<02:27,  7.75s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  10%|█         | 2/20 [00:15<02:19,  7.77s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  15%|█▌        | 3/20 [00:18<01:34,  5.58s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  20%|██        | 4/20 [00:22<01:17,  4.87s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  25%|██▌       | 5/20 [00:27<01:13,  4.89s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  30%|███       | 6/20 [00:30<01:00,  4.33s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  35%|███▌      | 7/20 [00:37<01:09,  5.32s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  40%|████      | 8/20 [00:43<01:04,  5.35s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  45%|████▌     | 9/20 [00:49<01:02,  5.64s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  50%|█████     | 10/20 [01:03<01:21,  8.13s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  55%|█████▌    | 11/20 [01:08<01:06,  7.38s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  60%|██████    | 12/20 [01:13<00:52,  6.59s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  65%|██████▌   | 13/20 [01:19<00:44,  6.35s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  70%|███████   | 14/20 [01:24<00:35,  5.91s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  75%|███████▌  | 15/20 [01:28<00:26,  5.35s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  80%|████████  | 16/20 [01:31<00:18,  4.72s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  85%|████████▌ | 17/20 [01:35<00:13,  4.43s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  90%|█████████ | 18/20 [01:38<00:07,  3.96s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]:  95%|█████████▌| 19/20 [01:40<00:03,  3.56s/it]Llama.generate: prefix-match hit\n",
      "Input: The [MASK] brown [MASK]: 100%|██████████| 20/20 [01:43<00:00,  5.19s/it]\n",
      "Input: The quick [MASK] [MASK]:   0%|          | 0/20 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:   5%|▌         | 1/20 [00:06<02:00,  6.36s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  10%|█         | 2/20 [00:10<01:33,  5.19s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  15%|█▌        | 3/20 [00:15<01:22,  4.87s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  20%|██        | 4/20 [00:18<01:08,  4.29s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  25%|██▌       | 5/20 [00:24<01:10,  4.69s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  30%|███       | 6/20 [00:29<01:08,  4.92s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  35%|███▌      | 7/20 [00:35<01:10,  5.44s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  40%|████      | 8/20 [00:45<01:19,  6.65s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  45%|████▌     | 9/20 [00:51<01:10,  6.41s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  50%|█████     | 10/20 [00:55<00:58,  5.85s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  55%|█████▌    | 11/20 [00:59<00:46,  5.16s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  60%|██████    | 12/20 [01:02<00:36,  4.58s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  65%|██████▌   | 13/20 [01:06<00:30,  4.34s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  70%|███████   | 14/20 [01:11<00:26,  4.47s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  75%|███████▌  | 15/20 [01:14<00:21,  4.29s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  80%|████████  | 16/20 [01:18<00:16,  4.04s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  85%|████████▌ | 17/20 [01:21<00:11,  3.83s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  90%|█████████ | 18/20 [01:25<00:07,  3.94s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]:  95%|█████████▌| 19/20 [01:29<00:03,  3.97s/it]Llama.generate: prefix-match hit\n",
      "Input: The quick [MASK] [MASK]: 100%|██████████| 20/20 [01:35<00:00,  4.76s/it]\n"
     ]
    }
   ],
   "source": [
    "from tools.command_generator import generate_prompts, prefix_prompt\n",
    "from tools.evaluate_response import get_replacements\n",
    "from tqdm import tqdm\n",
    "# takes 23 minutes to run with the sentence I wrote. Make a shorter sentence to run faster.\n",
    "# out comment this when the code has run once and you saved the responses.\n",
    "def run_prompts(model, sentence, anchor_idx, prompts_per_word=20):\n",
    "    prompts = generate_prompts(sentence, anchor_idx)\n",
    "    all_replacements = []\n",
    "    for prompt in prompts:\n",
    "        replacements = []\n",
    "        for _ in tqdm(\n",
    "            range(prompts_per_word),\n",
    "            desc=f\"Input: {prompt}\",\n",
    "        ):\n",
    "            response = model.get_response(\n",
    "                prefix_prompt(prompt),\n",
    "            ).strip()\n",
    "            if response:\n",
    "                replacement = get_replacements(prompt, response)\n",
    "                if replacement:\n",
    "                    replacements.append(replacement)\n",
    "        if len(replacements) > 0:\n",
    "            all_replacements.append(replacements)\n",
    "    return all_replacements\n",
    "\n",
    "all_responses = run_prompts(model, sentence, anchor_word_idx, prompts_per_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['the', 'fox jumps over the lazy dog'], ['the fox jumped over the', 'fence'], ['fox', 'fox'], ['the', 'fox jumps over the lazy dog'], ['the', 'dog jumps'], ['the', 'fox'], ['the', 'fox jumps over the lazy dog'], ['the', 'fox jumps'], ['', ''], ['the fox is', 'brown'], ['the', 'dog'], ['', ''], ['the fox was [mask]', '[mask] jumps over the lazy dog'], ['the', 'fox jumps'], ['the', 'fox jumps'], ['the', 'fox jumps'], ['the', 'fox jumps'], ['the fox was very', 'the brown dog followed close behind'], ['the', 'dog jumped'], ['the', 'dog']], [['dog', 'fur'], ['beautiful', 'deer'], ['chocolate', 'dog'], ['cat', 'coat'], ['deer', 'coat'], ['deer', 'coat'], ['beautiful', 'cat'], ['dog', 'coat'], ['dog', 'coat'], ['dog', 'fur'], ['cat', 'tabby'], ['elephant', 'tusks'], ['dog', 'fur'], ['dog', 'furred'], ['beautiful', 'dog'], ['cow', 'meadow'], ['friendly', 'dog'], ['bear', 'coat'], ['beautiful', 'deer'], ['bear', 'coat']], [['fox', 'runs'], ['fox', 'runs'], ['fox', 'jumps'], ['fox', 'runs'], ['brown', 'fox jumps'], ['fox', 'runs fast'], ['fox', 'jumps'], ['brown', 'dog'], ['fox', 'jumps'], ['fox', 'jumps'], ['brown', 'fox'], ['fox', 'ran'], ['fox', 'jumps'], ['brown', 'fox jumped'], ['fox', 'jumps'], ['brown', 'fox'], ['brown', 'fox'], ['fox', 'runs'], ['fox', 'jumps'], ['brown', 'fox jumps']]]\n"
     ]
    }
   ],
   "source": [
    "# out comment this when the code has run once and you saved the responses.\n",
    "\n",
    "print(all_responses)\n",
    "# save the responses to a file\n",
    "with open(\"responses.txt\", \"w\") as f:\n",
    "    f.write(str(all_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['the fox', 'fox'], ['the', 'fox'], ['fox', 'fox'], ['', 'fox'], ['fox', 'fox'], ['the', 'fox'], ['fox', 'fox'], ['the', 'fox'], ['fox', 'fox'], ['fox', 'fox'], ['', ''], ['the', 'fox'], ['the', 'fox'], ['the swift', 'fox'], ['', 'fox'], ['the', 'fox'], ['the fox', 'fox'], ['', 'fox'], ['the', 'fox'], ['the', 'fox']], [['fox', 'rabbit'], ['friendly', 'cat'], ['clever', 'rabbit'], ['gray', 'cat'], ['quick', 'fox'], ['', ''], ['quick', 'fox'], ['', ''], ['black', 'cat'], ['quick', 'fox'], ['quick', 'fox'], ['fox', 'fox'], ['yellow', 'cat'], ['yellow', 'cat'], ['little', 'cat'], ['yellow', 'fox'], ['fox', 'vixen'], ['old', 'cat'], ['tiny', 'frog'], ['feline', 'cat']], [['', ''], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['fox', 'cleverly'], ['brown', 'fox'], ['fox', '[mask]'], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['fox', '[mask]'], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['', ''], ['brown', 'fox']], [['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', '[leaps]'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumped'], ['fox', '[jumps]'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps']], [['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over']], [['fox', 'the'], ['fox', 'the'], ['fox', 'the'], ['fox', ''], ['fox', ''], ['fox', 'the'], ['fox', 'the'], ['fox', ''], ['fox', 'the'], ['fox', 'the'], ['fox', 'the'], ['fox', 'the'], ['fox', ''], ['fox', 'the'], ['fox', ''], ['fox', 'the'], ['fox', ''], ['fox', 'a'], ['fox', 'the'], ['fox', '']], [['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'sleeping'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'sleeping'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy']], [['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['dog', 'cat'], ['dog', 'fox'], ['dog', 'cat'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['', ''], ['dog', 'fox'], ['fox', 'dog'], ['fox', 'dog']]]\n"
     ]
    }
   ],
   "source": [
    "# just overwring the response and sentence if you chose to change it. You can change it to your sentence and configurations.\n",
    "\n",
    "# load the responses from a file\n",
    "with open(\"responses_Fox_and_Dog.txt\", \"r\") as f:\n",
    "    all_responses = eval(f.read())\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "anchor_word_idx = 3 # the index of the interested word\n",
    "prompts_per_word = 20 # number of generated responses  \n",
    "\n",
    "print(all_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['the fox', 'fox'], ['the', 'fox'], ['fox', 'fox'], ['', 'fox'], ['fox', 'fox'], ['the', 'fox'], ['fox', 'fox'], ['the', 'fox'], ['fox', 'fox'], ['fox', 'fox'], ['', ''], ['the', 'fox'], ['the', 'fox'], ['the swift', 'fox'], ['', 'fox'], ['the', 'fox'], ['the fox', 'fox'], ['', 'fox'], ['the', 'fox'], ['the', 'fox']], [['fox', 'rabbit'], ['friendly', 'cat'], ['clever', 'rabbit'], ['gray', 'cat'], ['quick', 'fox'], ['', ''], ['quick', 'fox'], ['', ''], ['black', 'cat'], ['quick', 'fox'], ['quick', 'fox'], ['fox', 'fox'], ['yellow', 'cat'], ['yellow', 'cat'], ['little', 'cat'], ['yellow', 'fox'], ['fox', 'vixen'], ['old', 'cat'], ['tiny', 'frog'], ['feline', 'cat']], [['', ''], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['fox', 'cleverly'], ['brown', 'fox'], ['fox', '[mask]'], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['fox', '[mask]'], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['brown', 'fox'], ['', ''], ['brown', 'fox']], [['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', '[leaps]'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumped'], ['fox', '[jumps]'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps'], ['fox', 'jumps']], [['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over'], ['fox', 'over']], [['fox', 'the'], ['fox', 'the'], ['fox', 'the'], ['fox', ''], ['fox', ''], ['fox', 'the'], ['fox', 'the'], ['fox', ''], ['fox', 'the'], ['fox', 'the'], ['fox', 'the'], ['fox', 'the'], ['fox', ''], ['fox', 'the'], ['fox', ''], ['fox', 'the'], ['fox', ''], ['fox', 'a'], ['fox', 'the'], ['fox', '']], [['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'sleeping'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'sleeping'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy']], [['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['dog', 'cat'], ['dog', 'fox'], ['dog', 'cat'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['', ''], ['dog', 'fox'], ['fox', 'dog'], ['fox', 'dog']]]\n",
      "8\n",
      "[['the fox', 'fox'], ['the', 'fox'], ['fox', 'fox'], ['', 'fox'], ['fox', 'fox'], ['the', 'fox'], ['fox', 'fox'], ['the', 'fox'], ['fox', 'fox'], ['fox', 'fox'], ['', ''], ['the', 'fox'], ['the', 'fox'], ['the swift', 'fox'], ['', 'fox'], ['the', 'fox'], ['the fox', 'fox'], ['', 'fox'], ['the', 'fox'], ['the', 'fox']]\n",
      "20\n",
      "['the fox', 'fox']\n",
      "2\n",
      "[['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['dog', 'cat'], ['dog', 'fox'], ['dog', 'cat'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['fox', 'dog'], ['', ''], ['dog', 'fox'], ['fox', 'dog'], ['fox', 'dog']]\n",
      "20\n",
      "['fox', 'dog']\n",
      "2\n",
      "[['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'sleeping'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'sleeping'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy'], ['fox', 'lazy']]\n",
      "20\n",
      "['fox', 'lazy']\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(all_responses)\\nprint(len(all_responses))\\nprint(all_responses[8])\\nprint(len(all_responses[8]))\\nprint(all_responses[8][0])\\nprint(len(all_responses[8][0]))\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_responses)\n",
    "print(len(all_responses))\n",
    "print(all_responses[0])\n",
    "print(len(all_responses[0]))\n",
    "print(all_responses[0][0])\n",
    "print(len(all_responses[0][0]))\n",
    "\n",
    "\n",
    "print(all_responses[7])\n",
    "print(len(all_responses[7]))\n",
    "print(all_responses[7][0])\n",
    "print(len(all_responses[7][0]))\n",
    "\n",
    "\n",
    "\n",
    "print(all_responses[6])\n",
    "print(len(all_responses[6]))\n",
    "print(all_responses[6][0])\n",
    "print(len(all_responses[6][0]))\n",
    "\n",
    "# out of range because the sentence has 9 words, but this is exclusive the original masked word word\n",
    "\"\"\"\n",
    "print(all_responses)\n",
    "print(len(all_responses))\n",
    "print(all_responses[8])\n",
    "print(len(all_responses[8]))\n",
    "print(all_responses[8][0])\n",
    "print(len(all_responses[8][0]))\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We are looking at the sentence \"The quick brown fox jumps over the lazy dog\".\n",
    "> We start by masking the word 'fox'. \n",
    "> The first index in the all_responses corresponds to the word index in the sentence (exclusive the masked word ('fox)). Thus index 2 corresponds to the word 'brown' and the index 4 corresponds to the word 'jumps'.\n",
    "> We then mask the word with the index i and generate 20 responses for the two masked words in the sentence thus the second index in all responses corresponds each of these 20 generated responses.\n",
    ">\n",
    "> The responses are given in the order of the words in the sentence. So even though we mask out the word x='fox' in the sentence, the response for this word changes when we move from the left side of the sentence to the right side.\n",
    ">For index 0 the output is given as [y,x] = ['the', 'fox'] \n",
    ">For index 7 the output is given as [x,y] = ['fox', 'dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 EXERCISE: compute the PMI for each word\n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "* Compute the $P(x)$, $P(y)$ and $P(x,y)$ first and print it out.\n",
    "* Compute the PMI for each word.\n",
    "* Visualize the result by coloring. Tips: you might need to normalize the result first. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "fox\n",
      "['The', 'quick', 'brown', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "sentence_split = sentence.split()\n",
    "true_x = sentence_split[anchor_word_idx]\n",
    "print(sentence_split)\n",
    "print(true_x)\n",
    "sentence_split_masked_x = sentence_split.copy()\n",
    "sentence_split_masked_x.pop(anchor_word_idx)\n",
    "print(sentence_split_masked_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.25, 1: 0.15, 2: 0.15, 3: 1.0, 4: 1e-10, 5: 1e-10, 6: 1e-10, 7: 0.1}\n",
      "{0: 1e-10, 1: 1e-10, 2: 1e-10, 3: 0.85, 4: 1e-10, 5: 1e-10, 6: 1e-10, 7: 0.2}\n",
      "{0: 1e-10, 1: 1e-10, 2: 1e-10, 3: 0.85, 4: 1e-10, 5: 1e-10, 6: 1e-10, 7: 0.1}\n",
      "{0: 2.0, 1: 2.736965594166206, 2: 2.736965594166206, 3: 0.0, 4: 33.219280948873624, 5: 33.219280948873624, 6: 33.219280948873624, 7: 2.321928094887362}\n"
     ]
    }
   ],
   "source": [
    "# Your code here. You are more than welcome to build .py file and run with that.\n",
    "\n",
    "P_x = {}\n",
    "P_y = {}\n",
    "P_x_y = {}\n",
    "PMI = {}\n",
    "epsilon = 1e-10\n",
    "\n",
    "\n",
    "for i in range(len(all_responses)):\n",
    "    true_y = sentence_split_masked_x[i]\n",
    "    P_x[i] = 0\n",
    "    P_y[i] = 0\n",
    "    P_x_y[i] = 0\n",
    "    for j in range(len(all_responses[i])):\n",
    "        if i > anchor_word_idx:\n",
    "            if all_responses[i][j][1] == true_x:\n",
    "                P_x[i] += 1\n",
    "            if all_responses[i][j][0] == true_y:\n",
    "                P_y[i] += 1\n",
    "            if all_responses[i][j][1] == true_x and all_responses[i][j][0] == true_y:\n",
    "                P_x_y[i] += 1\n",
    "        else:\n",
    "            if all_responses[i][j][0] == true_x:\n",
    "                P_x[i] += 1\n",
    "            if all_responses[i][j][1] == true_y:\n",
    "                P_y[i] += 1\n",
    "            if all_responses[i][j][0] == true_x and all_responses[i][j][1] == true_y:\n",
    "                P_x_y[i] += 1\n",
    "            \n",
    "    P_x[i] = P_x[i]/len(all_responses[i])\n",
    "    P_y[i] = P_y[i]/len(all_responses[i])\n",
    "    P_x_y[i] = P_x_y[i]/len(all_responses[i])\n",
    "\n",
    "    if P_x[i] == 0:\n",
    "        P_x[i] = epsilon\n",
    "    if P_y[i] == 0:\n",
    "        P_y[i] = epsilon\n",
    "    if P_x_y[i] == 0:\n",
    "        P_x_y[i] = epsilon\n",
    "\n",
    "\n",
    "    PMI[i] = np.log2(P_x_y[i]/(P_x[i]*P_y[i]))\n",
    "\n",
    "print(P_x)\n",
    "print(P_y)\n",
    "print(P_x_y)\n",
    "print(PMI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. EXERCISE: Try more examples; maybe come up with your own. Report the results.\n",
    "\n",
    "* Try to come up with more examples and, change the anchor word/number of responses, and observe the results. What does the explanation mean? Do you think it's a nice explanation? Why and why not? \n",
    "* What's the limitation of the current method? When does the method fail to explain? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus Exercises\n",
    "### 5.1 Language pre-processing. \n",
    "In this exercise, we only lower the letters and split sentences into words; there's much more to do to pre-process the language. For example, contractions (*I'll*, *She's*, *world's*), suffix and prefix, compound words (*hard-working*). It's called word tokenization in NLP, and there are some Python packages that can do such work for us, e.g. [*TextBlob*](https://textblob.readthedocs.io/en/dev/). \n",
    "\n",
    "\n",
    "### 5.2 Better word matching\n",
    "In the above example of\n",
    "> Tokyo is the capital of Japan and a popular metropolis in the world.\n",
    "\n",
    ", GenAI never gives the specific word 'metropolis' when masking it out; instead, sometimes it provides words like 'city', which is not the same word but has a similar meaning. Instead of measuring the exact matching of certain words (i.e. 0 or 1), we can also measure the similarity of two words, e.g. the cosine similarity in word embedding, which ranges from 0 to 1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
